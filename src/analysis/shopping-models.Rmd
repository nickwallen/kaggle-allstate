```{r, include=FALSE}

library (data.table)
library (ggplot2)
library (reshape2)
library (lubridate)
library (stringr)
library (caret)
library (gridExtra)

source ("../R/fetch.R")
source ("../R/features.R")
source ("../R/shopping-models.R")
source ("../R/score.R")
source ("../R/utils.R")
source ("../R/cache.R")

# set global options for all code chunks
opts_chunk$set(fig.width=12, fig.height=8, tidy=T)
```

Modeling of Customer Shopping History
========================================================

The customer demographic data and the customer shopping history are completely different types of data.  We are going to model each of these independently.  The final prediction will be an ensemble between the two.  The initial challenge for the shopping history data is in flattening a customer's shopping history into one single record that can be used to train and predict the customer's final purchase plan.  

### Preprocessing

The input shopping data looks like the following.

| customer.id | shopping.pnt | record.type | day.of.week | time.of.day.hours | option.a | option.b | ... |
| ----------- | ------------ | ----------- | ----------- | ----------------- | -------- | -------- | ----|                                                 
| 123456789   | 1            | shopping    | Mon         | 2.25              | 0        | 1        |     |
| 123456789   | 2            | shopping    | Mon         | 3.25              | 0        | 1        |     |
| 123456789   | 3            | shopping    | Mon         | 3.50              | 1        | 2        |     |
| 123456789   | 4            | shopping    | Mon         | 4.00              | 1        | 2        |     |
| 123456789   | 5            | shopping    | Mon         | 5.00              | 0        | 2        |     |
| 123456789   | 6            | purchase    | Mon         | 6.26              | 0        | 2        |     |
| 000000001   | 1            | shopping    | Tue         | 2.25              | 0        | 1        |     |
| 000000001   | 2            | shopping    | Tue         | 3.25              | 0        | 1        |     |
| 000000001   | 3            | purchase    | Tue         | 3.50              | 1        | 2        |     |

The input data could be transformed as follows for training and prediction.

| customer.id | option.a.0 | option.a.1 | option.a.2 | option.b.0 | option.b.1 | option.a | option.b | ... | 
| ----------- | ---------- | ---------- | ---------- | ---------- | ---------- | -------- | -------- | --- |
| 123456789   | 3          | 2          | 0          | 2          | 2          | 0        | 2        |     |
| 000000001   | 2          | 0          | 0          | 0          | 2          | 1        | 2        |     |

The definition for each of these fields is as follows.

| Field         | Definition 
| ------------- | ---------- 
| *customer.id* | The customer's unique ID.  Each customer will have one record in training data.       
| *option.a.0*  | The total number of times the customer shopped for choice 0 of option A in the shopping history.
| *option.a.1*  | ...
| *option.a.2*  | ...
| *option.b.0*  | The total number of times the customer shopped for choice 0 of option A in the shopping history.
| *option.b.1*  | ...
| *option.a*    | The outcome for option a that is being predicted.  The value for this field comes from the purchase record.
| *option.b*    | ...

```{r, include=F}
competition.train <- fetch()
```

### Popular Model

Let's first look at a popular model.  The popular model simply chooses the option that has been shopped for by most
customers.  If a customer looks at the same option multiple times, then the option receives multiple votes.

```{r popular.model, results='hide'}

# sums the number of times a customer has shopped for a particular option (A0, A1, A2, ..., B0, B1, ...)
shopping <- extract.shopping.history (competition.train, sum)

# split the data into representative test/train sets
add.plan (shopping)
train.index <- createDataPartition (shopping$plan, p = 0.8, list = F)[,1]

# create a train and test set
shopping.train <- shopping [  train.index ]
shopping.test  <- shopping [ -train.index ]

# train the naive model
fit <- popular.model (shopping.train)

# make predictions with the naive model
shopping.test [, options.hat() := predict (fit, .SD), with = F]
options.hat.as.factors (shopping.test)

```

How did the popular model perform?

```{r popular.model.score}

# score the results
accuracy.score (shopping.test)
partial.accuracy.score (shopping.test)

```

Take a closer look using a confusion matrix for each option.

```{r popular.model.confusion.matrix}
with (shopping.test, confusionMatrix(option.a.hat, option.a))
with (shopping.test, confusionMatrix(option.b.hat, option.b))
with (shopping.test, confusionMatrix(option.c.hat, option.c))
with (shopping.test, confusionMatrix(option.d.hat, option.d))
with (shopping.test, confusionMatrix(option.e.hat, option.e))
with (shopping.test, confusionMatrix(option.f.hat, option.f))
with (shopping.test, confusionMatrix(option.g.hat, option.g))
```

### Shopping Counts

This model uses a classification tree to predict the final purchased option based on the number of times that customer has shopped for a particular valid option.  In addition, each option (a-g) is modeled separately.

```{r shopping.counts.model}

# fetch the competition training data set and transform it for training
input <- fetch()
shopping.train <- extract.shopping.history (input, sum)
shopping.train <- extract.purchase.history (input, shopping.train)

# split the data into representative test/train sets
add.plan (shopping)
train.index <- createDataPartition (shopping$plan, p = 0.8, list = F)[,1]

# create a train and test set
shopping.train <- shopping [  train.index ]
shopping.test  <- shopping [ -train.index ]

# defines how the parameter tuning will occur
control <- trainControl (method = "cv", number = 2)

# which model parameters will be tuned?
tune.grid <- expand.grid (
  n.trees           = c(50, 100, 200), 
  shrinkage         = 0.1,
  interaction.depth = c(1, 5, 9))

# tune/train a separate model for each option
models <- lapply (options(), function (option) {
  train (
    method    = "gbm", 
    trControl = control,
    y         = shopping.train [[option]],
    x         = shopping.train [, 2:23, with = F],
    tuneGrid  = tune.grid,
    verbose   = FALSE )
})

# name each of the models in the list
names(models) <- options.hat()

```

How did the parameter tuning go?

```{r shopping.counts.tuning}

# create tuning plots for each model
plots <- lapply (models, function (m) ggplot (m))

# draw each of the plots on the same canvas
do.call(grid.arrange,  plots)

```

Which predictors are most important?  The results seem to show that a customer shopping for a particular option is indicative of what they will eventually purchase.  In addition it does not seem like the selection across option types has any significant impact.  For example, the selection of option A.1 does not impact E.2 or anything like that.

```{r shopping.counts.importance.plots}

# create variable importance plots for each model
plots <- lapply (models, function (m) plot (varImp (m, scale = F), top = 10))

# draw each of the plots on the same canvas
do.call(grid.arrange,  plots)

```

Make predictions with the various models.

```{r shopping.counts.predictions, results='hide'}

for (option.hat in names (models)) {

  # make a prediction for each option
  model <- models[[option.hat]]
  shopping.test [, option.hat := predict (model, .SD), 
                 .SDcols = 2:23, with = FALSE ]
}
```

How accurate are the predictions?

```{r shopping.counts.accuracy}

# score the results
accuracy.score (shopping.test)
partial.accuracy.score (shopping.test)

```

Take a closer look using a confusion matrix for each option.

```{r shopping.counts.confusion.matrix}

with (shopping.test, confusionMatrix(option.a.hat, option.a))
with (shopping.test, confusionMatrix(option.b.hat, option.b))
with (shopping.test, confusionMatrix(option.c.hat, option.c))
with (shopping.test, confusionMatrix(option.d.hat, option.d))
with (shopping.test, confusionMatrix(option.e.hat, option.e))
with (shopping.test, confusionMatrix(option.f.hat, option.f))
with (shopping.test, confusionMatrix(option.g.hat, option.g))
```

Run the models on the actual competition test data to create a submission.

```{r}
champion.shopping.model (verbose = TRUE)
```

### Customers Who Purchase Unquoted Policies

Could we determine which customers are likely to purchase policies for which they never received a quote?  These are customers who purchase options different from their last shopping point.  For all customers who are likely to purchase something without a previous quote, then predict using the data.  For all other customers, just choose the latest shopping point.  This might show a marked improvement since the naive model seems difficult to beat by simply making predictions for every customer.

Could also use the 'shopping.pt' value of the latest quote which matches the purchase.  Then '0' might indicate that no quote matches the purchase.

```{r}
train <- fetch ()
last.quote (train)
add.plan (train)
```

What is the distribution of 'last quote'?  Most often there has never been a quote for exactly what was purchased.

```{r}
ggplot (train, aes (last.quote)) + geom_histogram()
```

Is it more likely for the customer to purchase something they were quoted for?  Most of the time customers purchase a policy for which they were previously quoted.

```{r}
ggplot (train, aes (last.quote == 0)) + geom_histogram()
table (train$last.quote == 0)

with (train, table (plan, last.quote == 0))
ggplot (train, aes (last.quote == 0)) + geom_histo() + facet_wrap( ~ option.a, ncol = 4)
ggplot (train, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.b, ncol = 4)
ggplot (train, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.c, ncol = 4)
ggplot (train, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.d, ncol = 4)
ggplot (train, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.e, ncol = 4)
ggplot (train, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.f, ncol = 4)
ggplot (train, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.g, ncol = 4)
```

### Recency of Shopping Points

Perhaps a difference value for each of the 'option.x.n' fields could be used.  Perhaps ones that applies a greater weight to more recent shopping history or the mean instead of the sum.

There does appear to be some improvement when the recency of shopping history is taken into account.  It is is not a major improvement, but it helps.  Compare the following.

```{r}
champion.shopping.model (mean)
champion.shopping.model (weighted.sum.most.recent)
```


### TODO 

This takes care of the option elements of the shopping history, but how should the additional date-related 'day.of.week' and 'time.of.day.hours' fields be leveraged in the analysis.






