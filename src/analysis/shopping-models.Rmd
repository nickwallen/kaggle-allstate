```{r, include=FALSE}

library (data.table)
library (ggplot2)
library (reshape2)
library (lubridate)
library (stringr)
library (caret)
library (gridExtra)

source ("../R/fetch.R")
source ("../R/features.R")
source ("../R/alpha-shopping-model.R")
source ("../R/naive-shopping-model.R")
source ("../R/popular-shopping-model.R")
source ("../R/unquoted-shopping-model.R")
source ("../R/score.R")
source ("../R/utils.R")
source ("../R/cache.R")

# set global options for all code chunks
opts_chunk$set(fig.width=12, fig.height=8, tidy=T)
```

Modeling of Customer Shopping History
========================================================
The customer demographic data and the customer shopping history are completely different types of data.  We are going to model each of these independently.  The final prediction will be an ensemble between the two.  The initial challenge for the shopping history data is in flattening a customer's shopping history into one single record that can be used to train and predict the customer's final purchase plan.  

### Preprocessing

The input shopping data looks like the following.

| customer.id | shopping.pnt | record.type | day.of.week | time.of.day.hours | option.a | option.b | ... |
| ----------- | ------------ | ----------- | ----------- | ----------------- | -------- | -------- | ----|                                                 
| 123456789   | 1            | shopping    | Mon         | 2.25              | 0        | 1        |     |
| 123456789   | 2            | shopping    | Mon         | 3.25              | 0        | 1        |     |
| 123456789   | 3            | shopping    | Mon         | 3.50              | 1        | 2        |     |
| 123456789   | 4            | shopping    | Mon         | 4.00              | 1        | 2        |     |
| 123456789   | 5            | shopping    | Mon         | 5.00              | 0        | 2        |     |
| 123456789   | 6            | purchase    | Mon         | 6.26              | 0        | 2        |     |
| 000000001   | 1            | shopping    | Tue         | 2.25              | 0        | 1        |     |
| 000000001   | 2            | shopping    | Tue         | 3.25              | 0        | 1        |     |
| 000000001   | 3            | purchase    | Tue         | 3.50              | 1        | 2        |     |

The input data could be transformed as follows for training and prediction.

| customer.id | option.a.0 | option.a.1 | option.a.2 | option.b.0 | option.b.1 | option.a | option.b | ... | 
| ----------- | ---------- | ---------- | ---------- | ---------- | ---------- | -------- | -------- | --- |
| 123456789   | 3          | 2          | 0          | 2          | 2          | 0        | 2        |     |
| 000000001   | 2          | 0          | 0          | 0          | 2          | 1        | 2        |     |

The definition for each of these fields is as follows.

| Field         | Definition 
| ------------- | ---------- 
| *customer.id* | The customer's unique ID.  Each customer will have one record in training data.       
| *option.a.0*  | The total number of times the customer shopped for choice 0 of option A in the shopping history.
| *option.a.1*  | ...
| *option.a.2*  | ...
| *option.b.0*  | The total number of times the customer shopped for choice 0 of option A in the shopping history.
| *option.b.1*  | ...
| *option.a*    | The outcome for option a that is being predicted.  The value for this field comes from the purchase record.
| *option.b*    | ...

### Popular Model

Let's first look at a popular model.  The popular model simply chooses the option that has been shopped for by most
customers.  If a customer looks at the same option multiple times, then the option receives multiple votes.

```{r popular.model, results='hide'}

# split the data into representative test/train sets
data <- fetch()
add.plan (data)
train.index <- createDataPartition (data$plan, p = 0.8, list = F)[,1]

# create a train and test set
train <- data [  train.index ]
test  <- data [ -train.index ]

# preprocess the train and test sets
train <- train [record.type == "purchase"]
test  <- test  [record.type == "purchase"]

# train on the purchase records only
models <- train.popular.model (train )

# make predictions with the popular model
test [, options.hat() := predict.popular.model (models, .SD), with = F]

```

How did the popular model perform?

```{r popular.model.score}

# score the results
accuracy.score (test)
partial.accuracy.score (test)
```

Take a closer look using a confusion matrix for each option.

```{r popular.model.confusion.matrix}

with (test, confusionMatrix(option.a.hat, option.a))
with (test, confusionMatrix(option.b.hat, option.b))
with (test, confusionMatrix(option.c.hat, option.c))
with (test, confusionMatrix(option.d.hat, option.d))
with (test, confusionMatrix(option.e.hat, option.e))
with (test, confusionMatrix(option.f.hat, option.f))
with (test, confusionMatrix(option.g.hat, option.g))
```

### Alpha Model - Shopping History Only

This model uses a classification tree to predict the final purchased option based on the number of times that customer has shopped for a particular valid option.  In addition, each option (a-g) is modeled separately.

```{r shopping.counts.model}

# fetch the competition training data set and transform it for training
data <- fetch()
data.shopping <- extract.quotes (data, sum)
data.shopping <- extract.purchases (data, data.shopping)

# split the data into representative test/train sets
add.plan (data.shopping)
train.index <- createDataPartition (data.shopping$plan, p = 0.8, list = F)[,1]

# create a train and test set
train <- data.shopping [  train.index ]
test  <- data.shopping [ -train.index ]

models <- train.alpha.model (train, verbose = TRUE)
```

How did the parameter tuning go?

```{r shopping.counts.tuning}

# create tuning plots for each model
plots <- lapply (models, function (m) ggplot (m))

# draw each of the plots on the same canvas
do.call(grid.arrange,  plots)

```

Which predictors are most important?  The results seem to show that a customer shopping for a particular option is indicative of what they will eventually purchase.  In addition it does not seem like the selection across option types has any significant impact.  For example, the selection of option A.1 does not impact E.2 or anything like that.

```{r shopping.counts.importance.plots}

# create variable importance plots for each model
plots <- lapply (models, function (m) plot (varImp (m, scale = F), top = 10))

# draw each of the plots on the same canvas
do.call(grid.arrange,  plots)

```

Make predictions with the various models.

```{r shopping.counts.predictions, results='hide'}
predict.alpha.model (models, test)
```

How accurate are the predictions?

```{r shopping.counts.accuracy}

# score the results
accuracy.score (test)
partial.accuracy.score (test)

```

Take a closer look using a confusion matrix for each option.

```{r shopping.counts.confusion.matrix}

with (test, confusionMatrix(option.a.hat, option.a))
with (test, confusionMatrix(option.b.hat, option.b))
with (test, confusionMatrix(option.c.hat, option.c))
with (test, confusionMatrix(option.d.hat, option.d))
with (test, confusionMatrix(option.e.hat, option.e))
with (test, confusionMatrix(option.f.hat, option.f))
with (test, confusionMatrix(option.g.hat, option.g))
```

Run the models on the actual competition test data to create a submission.

```{r}
export.alpha.model (verbose = TRUE)
```

### Unquoted Purchase Model

Which customers are likely to purchase policies for which they never received a quote?  These are customers who, by definition, purchase options different from their last shopping point.  Can we model this and predict which customers are likely to make an unquoted purchase?

For all customers who are likely to purchase something without a previous quote, then predict using the data.  For all other customers, just choose the latest shopping point.  This might show a marked improvement since the naive model seems difficult to beat by simply making predictions for every customer.

Could also use the 'shopping.pt' value of the latest quote which matches the purchase.  Then '0' might indicate that no quote matches the purchase.

```{r}
data <- fetch ()
add.last.quote (data)
add.plan (data)
```

What is the distribution of 'last quote'?  Most often there has never been a quote for exactly what was purchased.

```{r}
ggplot (data, aes (last.quote)) + geom_histogram()
```

Is it more likely for the customer to purchase something they were quoted for?  Most of the time customers purchase a policy for which they were previously quoted.

```{r}
ggplot (data, aes (last.quote == 0)) + geom_histogram()
table (data$last.quote == 0)

with (data, table (plan, last.quote == 0))
ggplot (data, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.a)
ggplot (data, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.b)
ggplot (data, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.c)
ggplot (data, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.d)
ggplot (data, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.e)
ggplot (data, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.f)
ggplot (data, aes (last.quote == 0)) + geom_histogram() + facet_wrap( ~ option.g)
```

Can we predict which customers will buy unquoted policies?  'Unquoted Purchase'?

```{r}

# add a yes/no indicator instead of using the raw count from last.quote
data [, unquoted.purchase := (last.quote == 0) ]

# split the data into representative test/train sets
train.index <- createDataPartition (data$unquoted.purchase, p = 0.8, list = F)[,1]

# create a train and test set
train <- data [  train.index ]
test  <- data [ -train.index ]

# which model parameters will be tuned?
tune.grid <- expand.grid (
  n.trees           = c(50, 100, 200), 
  shrinkage         = 0.1,
  interaction.depth = c(1, 5, 9))

```

Predict unquoted purchases with the options values only.

```{r}

# tune/train a separate model using options only
model <- train (
  form      = as.factor(unquoted.purchase) ~ option.a + option.b + option.c + option.d + 
                                             option.e + option.f + option.g,
  data      = train,
  method    = "gbm", 
  trControl = trainControl (method = "cv", number = 2),
  tuneGrid  = tune.grid,
  verbose   = TRUE 
)

# make a prediction 
test [, unquoted.purchase.hat := predict (model, newdata = .SD) ]
with (test, confusionMatrix (unquoted.purchase, unquoted.purchase.hat)) 

# TODO - oversample where unquoted purchase is FALSE?

```

Predict unquoted purchases with options and additional customer attributes.

```{r}

# tune/train a separate model using options only
model <- train (
  form      = as.factor(unquoted.purchase) ~ 
    time.of.day.hours + cost + car.age + age.oldest + age.youngest + 
    duration.previous + option.g,
  data      = train,
  method    = "gbm", 
  trControl = trainControl (method = "cv", number = 2),
  tuneGrid  = tune.grid,
  verbose   = TRUE 
)

# make a prediction 
unquoted.purchase.hat <- predict (model, newdata = test)
#test [, unquoted.purchase.hat := predict (model, newdata = .SD) ]
with (test, confusionMatrix (unquoted.purchase, unquoted.purchase.hat)) 

# TODO - oversample where unquoted purchase is FALSE?

```

Which predictors are most important?

```{r}
varImp (model)

#   only 20 most important variables shown (out of 43)
# 
#                    Overall
# time.of.day.hours  100.000
# cost                37.315
# car.age             11.946
# age.oldest           9.388
# age.youngest         8.674
# duration.previous    8.200
# option.g3            5.801
# option.g4            4.486
# option.g2            4.421
# option.d3            3.692
# option.f2            2.626
# option.e1            2.623
# option.a2            2.574
# option.a1            2.338
# group.size           2.311
# day.of.week^4        2.260
# option.f1            2.083
# option.c4            1.508
# risk.factormissing   1.365
# car.valuee           1.328
```

### Combining Models

The naive model is relatively effective for the competition. Even the best competitors are beating it by only a few points. This indicates that in most cases customers purchase plans for which customers have previously (and most recently) received a quote. For these customers, the naive model might be the best model.

Alternatively, there are other customers who will choose to purchase an unquoted plan. For these customers an alternative shopping or customer-based model should be used to improve accuracy.  This could be accomplished by ensembling the naive model with another model and have the ensemble model decide when each model should be applied.

Lastly, there will be a group of customers for which we have no shopping history.  For these customers we could use a purely customer based model.  Is this true? 

#### If/Else Combination

The simplest way to combine models is to use the 'Unquoted Purchase' model.

```{r}
#
# naive model - model requires no training data
#
test  <- fetch (train = FALSE)

naive.models <- train.naive.model (test)
naive.predictions <- predict.naive.model (naive.models, test)

# extract the predictions
setkey (naive.predictions, "customer.id")
options.hat.as.numeric (naive.predictions)
setnames (naive.predictions, options.hat(), paste0 (options.hat(), ".naive"))

#
# alpha shopping model
#
train <- fetch (train = TRUE)
test  <- fetch (train = FALSE)

train.shopping <- extract.quotes (train)
train.shopping <- extract.purchases (train, train.shopping)

#alpha.models <- cache ("train.alpha.models", train.alpha.model (data.shopping, verbose = TRUE))
alpha.models <- train.alpha.model (train.shopping, verbose = TRUE)

test.shopping <- extract.quotes (test)
predict.alpha.model (alpha.models, test.shopping)

# extract the predictions
alpha.predictions <- test.shopping [, c("customer.id", options.hat()), with = FALSE]
options.hat.as.numeric (alpha.predictions)
setnames (alpha.predictions, options.hat(), paste0 (options.hat(), ".alpha"))

#
# unquoted purchase model 
#
train <- fetch (train = TRUE)
test  <- fetch (train = FALSE)

train <- preprocess.unquoted.model (train)
unquoted.model <- cache ("train.unquoted.model", train.unquoted.model (train))

test <- preprocess.unquoted.model (test, train = FALSE)
predict.unquoted.model (unquoted.model, test)

#unquoted.purchase.hat <- predict (unquoted.model, newdata = test)
unquoted.predictions <- test [, list (customer.id, unquoted.purchase.hat) ]

#
# add the actuals for comparison
#
#data <- fetch (train = FALSE)
#actuals <- data [record.type == "purchase", c("customer.id", options()), with = FALSE ]
#options.as.numeric (actuals)

# merge the predictions
#predictions <- alpha.predictions [ naive.predictions ][ unquoted.predictions ][ actuals ]
predictions <- alpha.predictions [ naive.predictions ][ unquoted.predictions ]

# choose the alpha model only if the customer is likely to make an unquoted purchase
predictions [, `:=` (
  option.a.hat = ifelse (unquoted.purchase.hat == "TRUE" & !is.na (option.a.hat.alpha), 
                         option.a.hat.alpha, option.a.hat.naive),
  option.b.hat = ifelse (unquoted.purchase.hat == "TRUE" & !is.na (option.b.hat.alpha), 
                         option.b.hat.alpha, option.b.hat.naive),
  option.c.hat = ifelse (unquoted.purchase.hat == "TRUE" & !is.na (option.c.hat.alpha), 
                         option.c.hat.alpha, option.c.hat.naive),
  option.d.hat = ifelse (unquoted.purchase.hat == "TRUE" & !is.na (option.d.hat.alpha), 
                         option.d.hat.alpha, option.d.hat.naive),
  option.e.hat = ifelse (unquoted.purchase.hat == "TRUE" & !is.na (option.e.hat.alpha), 
                         option.e.hat.alpha, option.e.hat.naive),
  option.f.hat = ifelse (unquoted.purchase.hat == "TRUE" & !is.na (option.f.hat.alpha), 
                         option.f.hat.alpha, option.f.hat.naive),
  option.g.hat = ifelse (unquoted.purchase.hat == "TRUE" & !is.na (option.g.hat.alpha), 
                         option.g.hat.alpha, option.g.hat.naive)
  ), by = customer.id ] 
             
# score the results
#accuracy.score (predictions)
#partial.accuracy.score (predictions)

create.submission (predictions, file = "../../submissions/red-swingline-predictions.csv")

```

#### Model of Models 

```

# TODO - combine the predictions from both models and train a subsequent model that looks like...
# option.a ~ <customer attributes> + ... + alpha.option.a.hat + naive.option.a.hat

  option.a ~ <customer attributes> 
    + alpha.model.option.a.hat
    + naive.model.option.a.hat
    + customer.model.option.a.hat
    + <length of shopping history> 
```


### Recency of Shopping Points

Perhaps a difference value for each of the 'option.x.n' fields could be used.  Perhaps ones that applies a greater weight to more recent shopping history or the mean instead of the sum.

There does appear to be some improvement when the recency of shopping history is taken into account.  It is is not a major improvement, but it helps.  Compare the following.

```{r}
export.alpha.model (verbose = T, summary.func = weighted.sum.most.recent)
```


### TODO 

This takes care of the option elements of the shopping history, but how should the additional date-related 'day.of.week' and 'time.of.day.hours' fields be leveraged in the analysis.






